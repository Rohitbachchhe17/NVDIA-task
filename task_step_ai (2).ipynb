{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1.web crawling with scrapy**"
      ],
      "metadata": {
        "id": "1G8P_ix2NO00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsA-hms3q_Eu"
      },
      "outputs": [],
      "source": [
        "# Install Scrapy and other necessary libraries\n",
        "!pip install scrapy\n",
        "!pip install scrapy-crawlera\n",
        "\n",
        "# Create a Scrapy project\n",
        "!scrapy startproject cuda_docs\n",
        "\n",
        "# Navigate to the project directory\n",
        "%cd cuda_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class CudaSpider(scrapy.Spider):\n",
        "    name = \"cuda\"\n",
        "    start_urls = ['https://docs.nvidia.com/cuda/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Extract text content and links\n",
        "        for link in response.css('a::attr(href)').getall():\n",
        "            if link and link.startswith('/cuda/'):\n",
        "                yield response.follow(link, self.parse)\n",
        "\n",
        "        yield {\n",
        "            'url': response.url,\n",
        "            'content': ' '.join(response.css('::text').getall())\n",
        "        }\n",
        "\n",
        "# Configure and run the crawler\n",
        "process = CrawlerProcess(settings={\n",
        "    \"FEEDS\": {\n",
        "        \"cuda_docs.json\": {\"format\": \"json\"},\n",
        "    },\n",
        "})\n",
        "\n",
        "process.crawl(CudaSpider)\n",
        "process.start()\n"
      ],
      "metadata": {
        "id": "0vWLYnPWsjBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Chunking and Embedding Creation**"
      ],
      "metadata": {
        "id": "0crzub7TNglx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sentence-transformers\n",
        "!pip install nltk\n",
        "\n",
        "import json\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the scraped data\n",
        "with open('cuda_docs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Chunk the data based on sentence similarity\n",
        "chunks = []\n",
        "for entry in data:\n",
        "    sentences = nltk.sent_tokenize(entry['content'])\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    clusters = util.community_detection(embeddings, min_community_size=2, threshold=0.75)\n",
        "\n",
        "    for cluster in clusters:\n",
        "        chunk = {\n",
        "            'url': entry['url'],\n",
        "            'content': ' '.join([sentences[i] for i in cluster])\n",
        "        }\n",
        "        chunks.append(chunk)\n"
      ],
      "metadata": {
        "id": "jGu7jvbPsr2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus\n"
      ],
      "metadata": {
        "id": "hWiMHB1gt_2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Vector Database Creation with Milvus**"
      ],
      "metadata": {
        "id": "mY3EzGaNNyEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall grpcio grpcio-tools pymilvus -y\n"
      ],
      "metadata": {
        "id": "zf5f-LvmzXJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grpcio==1.53.0 grpcio-tools==1.53.0\n",
        "!pip install pymilvus==2.2.11\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "hLiDxiE_zbtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Import required modules\n",
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Connect to Milvus\n",
        "try:\n",
        "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
        "    print(\"Connected to Milvus\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Milvus: {e}\")\n",
        "\n",
        "# Define the schema\n",
        "fields = [\n",
        "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
        "    FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=500)\n",
        "]\n",
        "\n",
        "schema = CollectionSchema(fields, description=\"CUDA documentation chunks\")\n",
        "\n",
        "# Create a collection if it doesn't exist\n",
        "collection_name = \"cuda_docs\"\n",
        "if utility.has_collection(collection_name):\n",
        "    collection = Collection(collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists.\")\n",
        "else:\n",
        "    collection = Collection(name=collection_name, schema=schema)\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "\n",
        "# Prepare the data\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "chunks = [\n",
        "    {\"content\": \"CUDA is a parallel computing platform and programming model.\", \"url\": \"https://docs.nvidia.com/cuda/cuda-introduction/index.html\"},\n",
        "    # Add more chunks as needed\n",
        "]\n",
        "\n",
        "ids = list(range(len(chunks)))\n",
        "embeddings = [model.encode(chunk['content']).tolist() for chunk in chunks]\n",
        "urls = [chunk['url'] for chunk in chunks]\n",
        "\n",
        "# Insert the data\n",
        "try:\n",
        "    data = [ids, embeddings, urls]\n",
        "    mr = collection.insert(data)\n",
        "    print(\"Data inserted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inserting data into Milvus: {e}\")\n",
        "\n",
        "# Check the number of entities\n",
        "print(f\"Number of entities in collection: {collection.num_entities}\")\n"
      ],
      "metadata": {
        "id": "7qFR6qDlzs60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus==2.2.11 sentence-transformers grpcio==1.53.0\n"
      ],
      "metadata": {
        "id": "luydM-XuzyHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Connect to Milvus\n",
        "try:\n",
        "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
        "    print(\"Connected to Milvus\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Milvus: {e}\")\n",
        "\n",
        "# Define the schema\n",
        "fields = [\n",
        "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
        "    FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=500)\n",
        "]\n",
        "\n",
        "schema = CollectionSchema(fields, description=\"CUDA documentation chunks\")\n",
        "\n",
        "# Create a collection if it doesn't exist\n",
        "collection_name = \"cuda_docs\"\n",
        "if utility.has_collection(collection_name):\n",
        "    collection = Collection(collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists.\")\n",
        "else:\n",
        "    collection = Collection(name=collection_name, schema=schema)\n",
        "    print(f\"Collection '{collection_name}' created.\")\n"
      ],
      "metadata": {
        "id": "77rc9PzC2Fs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Example data\n",
        "chunks = [\n",
        "    {\"content\": \"CUDA is a parallel computing platform and programming model.\", \"url\": \"https://docs.nvidia.com/cuda/cuda-introduction/index.html\"},\n",
        "    # Add more chunks as needed\n",
        "]\n",
        "\n",
        "# Prepare the data\n",
        "embeddings = [model.encode(chunk['content']).tolist() for chunk in chunks]\n",
        "urls = [chunk['url'] for chunk in chunks]\n",
        "\n",
        "# Insert the data into Milvus\n",
        "try:\n",
        "    data = [\n",
        "        [i for i in range(len(embeddings))],  # ids\n",
        "        embeddings,\n",
        "        urls\n",
        "    ]\n",
        "    mr = collection.insert(data)\n",
        "    print(\"Data inserted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inserting data into Milvus: {e}\")\n",
        "\n",
        "# Check the number of entities\n",
        "if collection:\n",
        "    print(f\"Number of entities in collection: {collection.num_entities}\")\n",
        "else:\n",
        "    print(\"Collection object is not defined.\")\n"
      ],
      "metadata": {
        "id": "ukP_uG6Y2I64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import utility\n",
        "\n",
        "# Load the collection\n",
        "collection.load()\n",
        "\n",
        "def retrieve(query, top_k=10):\n",
        "    query_embedding = model.encode(query)\n",
        "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "    results = collection.search(\n",
        "        data=[query_embedding],\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=top_k,\n",
        "        expr=None,\n",
        "        output_fields=[\"url\"]\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example query\n",
        "query = \"What is CUDA?\"\n",
        "results = retrieve(query)\n",
        "\n",
        "# Re-rank based on similarity\n",
        "def re_rank(results, query):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    scores = []\n",
        "\n",
        "    for result in results[0]:\n",
        "        embedding = result.entity.get(\"embedding\")\n",
        "        score = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "        scores.append((result, score))\n",
        "\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [result[0] for result in scores]\n",
        "\n",
        "re_ranked_results = re_rank(results, query)\n"
      ],
      "metadata": {
        "id": "bBj6RlHM2Psm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Retrieval and Re-ranking**"
      ],
      "metadata": {
        "id": "j0ju6aGRONqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs[0])  # Get the most likely beginning of answer\n",
        "    answer_end = torch.argmax(outputs[1]) + 1  # Get the most likely end of answer\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Get the context from the re-ranked results\n",
        "context = re_ranked_results[0].entity.get(\"content\")\n",
        "\n",
        "# Get the answer\n",
        "answer = answer_question(query, context)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "7-HtQLEG2g4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Question Answering with an LLM**"
      ],
      "metadata": {
        "id": "w2ujirpwOqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers package if not already installed\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the pretrained tokenizer and model for question answering\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Define a function to answer questions given a context\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # Get the model's predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the most likely start and end positions of the answer\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "    # Convert the token ids to tokens and then to a string answer\n",
        "    answer = tokenizer.decode(input_ids[answer_start:answer_end])\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example query and context (replace with actual retrieved content)\n",
        "query = \"What is CUDA?\"\n",
        "context = \"CUDA is a parallel computing platform and programming model.\"\n",
        "\n",
        "# Example re-ranked results (replace with actual retrieval logic)\n",
        "re_ranked_results = [\n",
        "    {\"entity\": {\"content\": context}}\n",
        "]\n",
        "\n",
        "# Get the context from the re-ranked results\n",
        "context = re_ranked_results[0][\"entity\"][\"content\"]\n",
        "\n",
        "# Get the answer to the query based on the context\n",
        "answer = answer_question(query, context)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "rS9yR30G4KIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py\n"
      ],
      "metadata": {
        "id": "RX5f5H3y3A6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. (Optional) User Interface with Streamlit**"
      ],
      "metadata": {
        "id": "EVM9vXEaOyJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a Streamlit app\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import json\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection\n",
        "\n",
        "# Load the necessary models and data\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Connect to Milvus\n",
        "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
        "collection = Collection(\"cuda_docs\")\n",
        "collection.load()\n",
        "\n",
        "def retrieve(query, top_k=10):\n",
        "    query_embedding = model.encode(query)\n",
        "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "    results = collection.search(\n",
        "        data=[query_embedding],\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=top_k,\n",
        "        expr=None,\n",
        "        output_fields=[\"url\"]\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "def re_rank(results, query):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    scores = []\n",
        "\n",
        "    for result in results[0]:\n",
        "        embedding = result.entity.get(\"embedding\")\n",
        "        score = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "        scores.append((result, score))\n",
        "\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [result[0] for result in scores]\n",
        "\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = qa_model(**inputs)\n",
        "    answer_start = torch.argmax(outputs[0])\n",
        "    answer_end = torch.argmax(outputs[1]) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"NVIDIA CUDA Documentation QA System\")\n",
        "\n",
        "query = st.text_input(\"Enter your query:\")\n",
        "if query:\n",
        "    results = retrieve(query)\n",
        "    re_ranked_results = re_rank(results, query)\n",
        "    context = re_ranked_results[0].entity.get(\"content\")\n",
        "    answer = answer_question(query, context)\n",
        "    st.write(f\"Answer: {answer}\")\n",
        "    st.write(f\"Context: {context}\")\n"
      ],
      "metadata": {
        "id": "0qRJEDdZ2vJ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}